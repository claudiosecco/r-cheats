---
title: Writing Efficient R Code 
output: 
  html_document:
    theme: default
    css: custom.css
    includes:
      after_body: footer.html
---

<style>
body{
  font-size: 16px;
  line-height: 24px;
}
.main-container {
  max-width: 1200px;
}
</style>

My personal notes from DataCamp's courses:

* [Writing Efficient R Code](https://www.datacamp.com/courses/writing-efficient-r-code)
* [Parallel Programming in R](https://learn.datacamp.com/courses/parallel-programming-in-r)

###### [Back to home](index.html)

## Contents
- [The Art of Benchmarking](#the-art-of-benchmarking)  
-- [General](#general)  
-- [Benchmarking](#benchmarking)  
- [Fine Tuning: Efficient Base R](#fine-tunning)  
-- [Memory allocation](#memory-allocation)  
-- [Vectorize your code](#vectorize-your-code)  
-- [Data frames vs matrices](#data-frames-vs-matrices)  
- [Diagnosing Problems: Code Profiling](#diagnosing-problems)  
- [Turbo Charged Code: Parallel Programming](#turbo-charged-code)

## The Art of Benchmarking {#the-art-of-benchmarking}
### General {#general}
It's unfair to compare R velocity with C velocity. Though C running code time is 
faster, R writing code is simpler and faster.

Simply keeping R up to date, is the first step to optimization.

- v2.0 Lazy loading; fast loading of data with minimal expense of system memory;
- v2.13 Speeding up functions with the byte compiler;
- v3.0 Support for large vectors;
- Main releases every April (e.g. 3.0, 3.1, 3.2)
- Smaller bug fixes throughout the year (e.g. 3.3.0, 3.3.1, 3.3.2)

```{r}
# Print the R version details using version
version
```

### Benchmarking {#benchmarking}
Read datasets files is a common task that we want to optimize. R provides a
native format ```.rds``` that, if used without compression, is suitable for fast
reading and writing.

```{r}
# How long does it take to read movies from CSV?
system.time(read.csv('data/movies.csv'))

# How long does it take to read movies from RDS?
system.time(readRDS('data/movies.rds'))
```

Using microbenckmark package:

```{r}
# Load the microbenchmark package
library(microbenchmark)

# Compare the two functions
compare <- microbenchmark(read.csv('data/movies.csv'), 
                          readRDS('data/movies.rds'), 
                          times = 10)

# Print compare
compare
```

Getting data and benchmarking my machine performance:

```{r}
# Load the benchmarkme package
library(benchmarkme)

# Assign the variable ram to the amount of RAM on this machine
ram <- get_ram()
ram

# Assign the variable cpu to the cpu specs
cpu <- get_cpu()
cpu

# Run the io benchmark
res <- benchmark_io(runs = 1, size = 5)

# Plot the results
plot(res)

# Contribute to community
# upload_results(res)
```

## Fine Tuning: Efficient Base R {#fine-tunning}
### Memory allocation {#memory-allocation}
Avoid growing a vector. Prefer to define it previously with correct
size and type. Each time you increase the size of a vector, R requests the OS 
for more memory, which causes the code run slowly.

```{r}
n <- 30000

# Slow code
growing <- function(n) {
    x <- NULL
    for(i in 1:n)
        x <- c(x, rnorm(1))
    x
}

# Fast code
pre_allocate <- function(n) {
    x <- numeric(n) # Pre-allocate
    for(i in 1:n) 
        x[i] <- rnorm(1)
    x
}

# Use <- with system.time() to store the results of performance measures
system.time(res_grow <- growing(n))
system.time(res_allocate <- pre_allocate(n))
```

### Vectorize your code {#vectorize-your-code}
Calling an native R function eventually leads to C or FORTRAN code, which are 
heavily optimized.

Use vectorized solution whenever possible:

```{r}
n <- 1e6
x <- vector("numeric", n)
microbenchmark(
  x <- rnorm(n), # vectorized solution
  {
    for(i in seq_along(x)) # not vectorized
      x[i] <- rnorm(1)
  },
  times = 10)
```

### Data frames vs matrices {#data-frames-vs-matrices}
Work with matrices whenever possible to optimize your code. Matrices provide
better performance, mainly when extracting rows.

```{r}
mat <- matrix(rnorm(100000), ncol = 1000)
df <- as.data.frame(mat)

# extracting columns
microbenchmark(mat[,1], df[,1])

# extracting rows
microbenchmark(mat[1,], df[1,])
```

## Diagnosing Problems: Code Profiling {#diagnosing-problems}
Use Profvis package to profile your code and find bottlenecks.

```{r}
library(profvis)
profvis({
  data(movies, package = "ggplot2movies") # Load data
  braveheart <- movies[7288,]
  movies <- movies[movies$Action == 1,]
  plot(movies$year, movies$rating, xlab = "Year", ylab="Rating")
  model <- loess(rating ~ year, data = movies) # loess regression line
  j <- order(movies$year)
  lines(movies$year[j], model$fitted[j], col="forestgreen", lwd=2)
  points(braveheart$year, braveheart$rating, pch = 21, bg = "steelblue", 
         cex = 3)
  })
```

## Turbo Charged Code: Parallel Programming {#turbo-charged-code}
Parallel package allow processing in multiple cores.
Not every routine can benefit, or even be executed, in parallel. Only loops with
no dependency among iterations.
Besides that, if you have few iterations, multi thread communication may not
compensate for using multiple cores.

```{r}
library(parallel)

play <- function() {
    total <- no_of_rolls <- 0
    while(total < 10) {
      total <- total + sample(1:6, 1)
  
      # If even. Reset to 0
      if(total %% 2 == 0) total <- 0 
      no_of_rolls <- no_of_rolls + 1
    }
    no_of_rolls
}

# Set the number of games to play
no_of_games <- 1e5

## Time serial version
system.time(serial <- sapply(1:no_of_games, function(i) play()))

## Set up cluster
cl <- makeCluster(2) # 2 cores
clusterExport(cl, "play")

## Time parallel version
system.time(par <- parSapply(cl, 1:no_of_games, function(i) play()))

## Stop cluster
stopCluster(cl)
```

An excelent example using:

* `foreach` loop structure
* parallelization via `doParallel` package
* exporting objects to clusters environments
* opening packages in clusters environments

```{r}
suppressPackageStartupMessages(library(dplyr))
library(stringr)
library(janeaustenr)
library(doParallel)
library(foreach)
library(microbenchmark)

janeausten_words <- function() {
  # Names of the six books contained in janeaustenr
  books <- austen_books()$book %>% unique %>% as.character
  # Vector of words from all six books
  words <- sapply(books, extract_words) %>% unlist
  words
}

extract_words <- function(book_name) {
  # extract the text of the book
  text <- subset(austen_books(), book == book_name)$text
  # extract words from the text and convert to lowercase
  str_extract_all(text, boundary("word")) %>% unlist %>% tolower
}

max_frequency <- function(letter, words, min_length = 1) {
  w <- select_words(letter, words = words, min_length = min_length)
  frequency <- table(w)    
  frequency[which.max(frequency)]
}

select_words <- function(letter, words, min_length = 1) {
  min_length_words <- words[nchar(words) >= min_length]
  grep(paste0("^", letter), min_length_words, value = TRUE)
}

# Vector of words from all six books
words <- janeausten_words()

# function that runs in parallel
freq_doPar <- function(cores, min_length = 5) {
  # Register a cluster of size cores
  registerDoParallel(cores = cores)
  
  # foreach loop
  foreach(let = letters, .combine = c, 
          .export = c('max_frequency', "select_words", "words"),
          .packages = c('janeaustenr', 'stringr')) %dopar%
    max_frequency(let, words = words, min_length = min_length)
}

# function that runs sequentially
freq_seq <- function(min_length = 5) {
  foreach(let = letters, .combine = c) %do%
    max_frequency(let, words = words, min_length = min_length)
}

microbenchmark(
  { res_seq <- freq_seq() },
  { res_par <- freq_doPar(2) },
  times = 1, 
  unit = 's')

all.equal(res_par, res_seq)

```

