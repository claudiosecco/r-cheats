---
title: Web data in R
output: 
  html_document:
    theme: readable
    css: custom.css
    includes:
      after_body: footer.html
---

<style>
body{
  font-size: 16px;
  line-height: 24px;
}
.main-container {
  max-width: 1200px;
}
</style>

My personal notes from 
[DataCamp's course](https://www.datacamp.com/courses/working-with-web-data-in-r)

###### [Back to home](index.html)

## Contents
- [Downloading Files and Using API Clients](#api-clients)  
-- [Introduction](#introduction)  
-- [APIs](#apis)  
- [Using httr to interact with APIs directly](#httr)  
- [Handling JSON and XML](#json-xml)  
- [Web scraping with XPATHs](#web-scraping)  


## Downloading Files and Using API Clients {#api-clients}
### Introduction {#introduction}
Read functions like read.csv(), real.delim() can accept urls in place of local
paths.
You can use download.file() to save a local copy.

```{r}
csv_url <- paste0("http://s3.amazonaws.com/assets.datacamp.com/production",
                  "/course_1561/datasets/chickwts.csv")

# Download the file with download.file()
download.file(url = csv_url, destfile = 'data/feed_data.csv')

# Read it in with read.csv()
csv_data <- read.csv('data/feed_data.csv')
```

### APIs {#apis}
R has several packages with APIs implementations. Google 'CRAN <website>'. 

Example with pageviews: client for Wikipedia's API

```{r}
# Load pageviews
library(pageviews)

# Get the pageviews for "Hadley Wickham"
hadley_pageviews <- article_pageviews(project = "en.wikipedia", 
                                      "Hadley Wickham")

# Examine the resulting object
str(hadley_pageviews)
```

## Using httr to interact with APIs directly {#httr}
Package httr helps to interact directly with APIs.

```{r}
library(httr)

url <- paste0("https://wikimedia.org/api/rest_v1/metrics/pageviews/",
              "per-article/en.wikipedia.org/all-access/all-agents/",
              "Hadley_Wickham/daily/20170101/20170102")

# Make a GET request to url and save the results
pageview_response <- GET(url)

# Call content() to retrieve the data the server sent back
pageview_data <- content(pageview_response)

# Examine the results with str()
str(pageview_data)
```

function httr::http_error() helps with response codes:

```{r}
fake_url <- "http://google.com/fakepagethatdoesnotexist"

# Make the GET request
request_result <- GET(fake_url)

# Check request_result
if(http_error(request_result)){
	warning("The request failed")
} else {
	content(request_result)
}
```

Use 'query' argument for parameters based APIs:

```{r}
# Create list with nationality and country elements
query_params <- list(nationality = 'americans', 
    country = 'antigua')
    
# Make parameter-based call to httpbin, with query_params
parameter_response <- GET('https://httpbin.org/get', query = query_params)

# Print parameter_response
parameter_response
```

Typical consumption of an API with traffic limiter:

```{r}
# Construct a vector of 2 URLs
urls <- c('http://httpbin.org/status/404', 'http://httpbin.org/status/301')

for(url in urls){
    # Send a GET request to url
    result <- GET(url)
    # Delay for 5 seconds between requests
    Sys.sleep(1)
}
```

A function tying all together:

```{r}
get_pageviews <- function(article_title){
  url <- paste(
    paste0("https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/",
           "en.wikipedia/all-access/all-agents"), 
    article_title, 
    "daily/2015100100/2015103100", 
    sep = "/"
  )   
  response <- GET(url, user_agent("my@email.com this is a test")) 
  # Is there an HTTP error?
  if(http_error(response)){ 
    # Throw an R error
    stop("the request failed") 
  }
  # Return the response's content
  content(response)
}
```

## Handling JSON and XML {#json-xml}
toJSON, fromJSON {jsonlite}:

```{r}
library(jsonlite)

# Stringify some data
jsoncars <- toJSON(mtcars[1:5,], pretty=TRUE)
jsoncars

# Parse it back
fromJSON(jsoncars)

## Not run: 
#retrieve data frame
data1 <- fromJSON("https://api.github.com/users/hadley/orgs")
names(data1)
data1$login

# Nested data frames:
data2 <- fromJSON("https://api.github.com/users/hadley/repos")
names(data2)
names(data2$owner)
data2$owner$login

# Flatten the data into a regular non-nested dataframe
names(flatten(data2))

# Flatten directly (more efficient):
data3 <- fromJSON("https://api.github.com/users/hadley/repos", flatten = TRUE)
identical(data3, flatten(data2))
```

dplyr::bind_rows() is a good helper to deal with lists parsed from JSON:

```{r}
library(dplyr)

url <- paste0("https://wikimedia.org/api/rest_v1/metrics/pageviews/",
              "per-article/en.wikipedia.org/all-access/all-agents/",
              "Hadley_Wickham/daily/20170101/20170102")

# Make a GET request to url and save the results
pageview_response <- GET(url)

# Call content() to retrieve the data the server sent back
pageview_data <- content(pageview_response)

# Examine the results with str()
str(pageview_data)

pageview_data[["items"]] %>% bind_rows()
```

Like jsonlite, you have xlm2:

```{r}
library(xml2)
cd <- read_xml(xml2_example("cd_catalog.xml"))
class(cd)
xml_structure(xml_child(cd, 1))

# working with xpaths
xml_find_all(cd, xpath = '/CATALOG/CD/ARTIST')

# create data frame
cds <- xml_find_all(cd, xpath = '/CATALOG/CD')
df <- data.frame(title = 
                   xml_text(xml_find_all(cd, xpath = '/CATALOG/CD/TITLE')),
                 artist = 
                   xml_text(xml_find_all(cd, xpath = '/CATALOG/CD/ARTIST')),
                 country =
                   xml_text(xml_find_all(cd, xpath = '/CATALOG/CD/COUNTRY')),
                 company =
                   xml_text(xml_find_all(cd, xpath = '/CATALOG/CD/COMPANY')),
                 price =
                   xml_double(xml_find_all(cd, xpath = '/CATALOG/CD/PRICE')),
                 year =
                   xml_integer(xml_find_all(cd, xpath = '/CATALOG/CD/YEAR')))
df
```

## Web scraping with XPATHs {#web-scraping}
Use package rvest to extract data from web html pages.

```{r}
library(rvest)

# Hadley Wickham's Wikipedia page
test_url <- "https://en.wikipedia.org/wiki/Hadley_Wickham"

# Read the URL stored as "test_url" with read_html()
test_xml <- read_html(test_url)
test_xml

# xpath to look for a class called vcard
xpath <- paste0("//*[contains(concat( \" \", @class, \" \" ),",
                " concat( \" \", \"vcard\", \" \" ))]")

# Use html_node() to grab the node with the XPATH
node <- html_node(x = test_xml, xpath = xpath)
node

# look for directly for class fn with css argument
page_name <- html_node(x = node, css = '.fn')
page_name

# Extract the text from page_name
page_title <- html_text(page_name)
page_title
```

Use rvest::html_table() to convert web pages tables to data.frames:

```{r}
wiki_table <- html_table(node)
colnames(wiki_table) <- c("key", "value")
cleaned_table <- subset(wiki_table, !key == '')
str(cleaned_table)
```

More examples with css argument

```{r}
# Select the table elements
html_nodes(test_xml, css = 'table')

# Select elements with class = "infobox"
html_nodes(test_xml, css = '.infobox')

# Select elements with id = "firstHeading"
html_nodes(test_xml, css = '#firstHeading')
```

Wrapping everything up in usable function to extract infobox from wikipedia's 
pages:

```{r}
library(httr)
library(rvest)
library(xml2)

get_infobox <- function(title){
  base_url <- "https://en.wikipedia.org/w/api.php"
  
  # Change "Hadley Wickham" to title
  query_params <- list(action = "parse", 
    page = title, 
    format = "xml")
  
  resp <- GET(url = base_url, query = query_params)
  resp_xml <- content(resp)
  
  page_html <- read_html(xml_text(resp_xml))
  infobox_element <- html_node(x = page_html, css =".infobox")
  page_name <- html_node(x = infobox_element, css = ".fn")
  page_title <- html_text(page_name)
  
  wiki_table <- html_table(infobox_element)
  colnames(wiki_table) <- c("key", "value")
  cleaned_table <- subset(wiki_table, !wiki_table$key == "")
  name_df <- data.frame(key = "Full name", value = page_title)
  wiki_table <- rbind(name_df, cleaned_table)
  
  wiki_table
}
```
